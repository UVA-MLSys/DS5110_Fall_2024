{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, re, json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path_pattern = re.compile(r\"result-partition-(?P<partition>[0-9]+MB)/(?P<data_size>[0-9]+GB)/(run )?(?P<run_no>[0-9]+)\")\n",
    "search_result = result_path_pattern.search(\"result-partition-75MB/1GB/1\")\n",
    "search_dict = search_result.groupdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"result-partition-[0-9]+MB/((total)|([0-9]+GB))/(run )?[0-9]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_event_details(event):\n",
    "    return event.get('executionSucceededEventDetails') or \\\n",
    "        event.get('executionFailedEventDetails') or \\\n",
    "        event.get('executionStartedEventDetails') or \\\n",
    "        event.get('stateEnteredEventDetails') or {}\n",
    "\n",
    "def extract_event_details(history, results, verbose):\n",
    "    state = 0\n",
    "    execution_start, execution_end = None, None\n",
    "    inference_start, inference_end = None, None\n",
    "    result_path = None\n",
    "\n",
    "    # Print execution events\n",
    "    for event in history['events']:\n",
    "        timestamp = event['timestamp']\n",
    "        event_type = event['type']\n",
    "        details = get_event_details(event)\n",
    "        \n",
    "        if verbose:print(f\"{timestamp} - {event_type}\")\n",
    "        if details is not None and verbose:\n",
    "            print(f\"  Details: {details}\")\n",
    "            \n",
    "        try:\n",
    "            if event_type == 'ExecutionStarted': \n",
    "                execution_start = timestamp\n",
    "            elif event_type == 'ExecutionSucceeded': execution_end = timestamp\n",
    "            elif event_type == 'TaskStateEntered': \n",
    "                if state ==0:\n",
    "                    input_json = json.loads(details['input'])\n",
    "                    result_path = input_json['result_path']\n",
    "                    # print(f\"{timestamp}: Result Path: {result_path}\")\n",
    "                \n",
    "                    # if pattern.fullmatch(result_path) is None:\n",
    "                    if \"demo\" in result_path or result_path in results['result_path']:\n",
    "                        # print(f\"Skipping {result_path}.\")\n",
    "                        return results\n",
    "                    \n",
    "                    result_path_splitted = result_path.split('/')\n",
    "                    # \"result-partition-100MB/1GB/1\" or \"result-partition-100MB/1GB/run 1\"\n",
    "                    \n",
    "                    if 'Batches' in result_path:\n",
    "                        data_size = result_path_splitted[-4]\n",
    "                    else:\n",
    "                        data_size = result_path_splitted[-2]\n",
    "                    if data_size == 'total': data_size = '12.6GB'\n",
    "                    \n",
    "                    run_no = result_path_splitted[-1].replace('run ', '')\n",
    "                    partition_size = input_json['data_prefix']\n",
    "                    batch_size = input_json['batch_size']\n",
    "                \n",
    "                state += 1\n",
    "            elif event_type == 'MapStateEntered':\n",
    "                # input_json = json.loads(details['input'])\n",
    "                 \n",
    "                inference_start = timestamp\n",
    "                state += 1\n",
    "            elif event_type == 'MapStateExited': \n",
    "                inference_end = timestamp\n",
    "                state += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error {e.with_traceback}\")\n",
    "            return results\n",
    "            \n",
    "    if verbose: print(\"-\" * 80)\n",
    "    \n",
    "    delta = (execution_end - execution_start)\n",
    "    total_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n",
    "    if verbose: print(f\"Total Duration: {total_duration:.2f} seconds.\")\n",
    "\n",
    "    delta = (inference_end - inference_start)\n",
    "    inference_duration = delta.seconds +  delta.microseconds / 1e6 # convert to seconds\n",
    "    if verbose: print(f\"Inference Duration: {inference_duration:.2f} seconds.\")\n",
    "\n",
    "    results['result_path'].append(result_path)\n",
    "    results['data (GB)'].append(data_size)\n",
    "    results['run'].append(run_no)\n",
    "    results['partition (MB)'].append(partition_size)\n",
    "    results['total_duration (s)'].append(total_duration)\n",
    "    results['inference_duration (s)'].append(inference_duration)\n",
    "    results['batch_size'].append(batch_size)\n",
    "    results['batch_varying'].append('Batches' in result_path)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_step_function_logs(\n",
    "    state_machine_arn, start_date=None, end_date=None,\n",
    "    verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Collects the events log for a specific AWS Step Functions state machine.\n",
    "\n",
    "    :param state_machine_arn: The ARN of the Step Functions state machine\n",
    "    :param start_date: Optional, filter logs starting from this date (timezone-aware datetime object)\n",
    "    :param end_date: Optional, filter logs until this date (timezone-aware datetime object)\n",
    "    \"\"\"\n",
    "    # Initialize boto3 client for Step Functions\n",
    "    stepfunctions_client = boto3.client('stepfunctions')\n",
    "    results = {\n",
    "        key: [] for key in [\n",
    "            'result_path', 'data (GB)','batch_size', 'run', \n",
    "            'partition (MB)', 'total_duration (s)', \n",
    "            'inference_duration (s)', 'batch_varying']\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # List executions for the state machine\n",
    "        executions = stepfunctions_client.list_executions(\n",
    "            stateMachineArn=state_machine_arn,\n",
    "            statusFilter='SUCCEEDED',  # You can filter by RUNNING, FAILED, etc.\n",
    "            maxResults=1000 # update if you have more than 1000 executions\n",
    "        )\n",
    "\n",
    "        print(f\"Fetching logs for Step Functions state machine: {state_machine_arn}\\n\")\n",
    "        print(f\"Found {len(executions['executions'])} executions.\\n\")\n",
    "\n",
    "        # Iterate through executions\n",
    "        for execution in executions['executions']:\n",
    "            execution_arn = execution['executionArn']\n",
    "            start_time = execution['startDate']\n",
    "            stop_time = execution['stopDate']\n",
    "            \n",
    "            # Ensure start_date and end_date are timezone-aware and in UTC\n",
    "            if start_date:\n",
    "                start_date = start_date.astimezone(pytz.utc)\n",
    "            if end_date:\n",
    "                end_date = end_date.astimezone(pytz.utc)\n",
    "\n",
    "            # Filter by start_date and end_date if provided\n",
    "            if start_date and start_time < start_date:\n",
    "                continue\n",
    "            if end_date and stop_time > end_date:\n",
    "                continue\n",
    "            \n",
    "            if verbose: print(f\"Execution ARN: {execution_arn}\")\n",
    "            if verbose: print(f\"Start Time: {start_time}, Stop Time: {stop_time}\")\n",
    "\n",
    "            # Get execution history\n",
    "            history = stepfunctions_client.get_execution_history(\n",
    "                executionArn=execution_arn,\n",
    "                reverseOrder=False  # Set to True if you want events in reverse order\n",
    "            )\n",
    "            \n",
    "            results = extract_event_details(history, results, verbose)\n",
    "            # break\n",
    "            \n",
    "    except stepfunctions_client.exceptions.ResourceNotFoundException:\n",
    "        print(f\"The state machine ARN {state_machine_arn} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    del results['result_path']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching logs for Step Functions state machine: arn:aws:states:us-east-1:211125778552:stateMachine:DataParallel-CosmicAI\n",
      "\n",
      "Found 229 executions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_machine_arn = \"arn:aws:states:us-east-1:211125778552:stateMachine:DataParallel-CosmicAI\"\n",
    "\n",
    "# Optional: Define a time range (use timezone-aware UTC datetimes)\n",
    "start_date = datetime(2024, 11, 11, 0, 0, 0, tzinfo=pytz.utc)  # Example: Start from this date\n",
    "end_date = None # datetime(2024, 11, 22, 23, 0, 0, tzinfo=pytz.utc)    # Example: Until this date\n",
    "\n",
    "results = get_step_function_logs(state_machine_arn, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data (GB)</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>run</th>\n",
       "      <th>partition (MB)</th>\n",
       "      <th>total_duration (s)</th>\n",
       "      <th>inference_duration (s)</th>\n",
       "      <th>batch_varying</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1GB</td>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>25MB</td>\n",
       "      <td>19.762</td>\n",
       "      <td>15.319</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1GB</td>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>25MB</td>\n",
       "      <td>20.023</td>\n",
       "      <td>15.644</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1GB</td>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>25MB</td>\n",
       "      <td>20.059</td>\n",
       "      <td>15.646</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  data (GB)  batch_size run partition (MB)  total_duration (s)  \\\n",
       "0       1GB         512   2           25MB              19.762   \n",
       "1       1GB         512   1           25MB              20.023   \n",
       "2       1GB         512   3           25MB              20.059   \n",
       "\n",
       "   inference_duration (s)  batch_varying  \n",
       "0                  15.319          False  \n",
       "1                  15.644          False  \n",
       "2                  15.646           True  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "df = df[(df['data (GB)'] != '100MB') & (df['partition (MB)'] != 'data')]\n",
    "# because the first ones are latest\n",
    "df.drop_duplicates(subset=['data (GB)', 'batch_size', 'run', 'partition (MB)', 'batch_varying'], inplace=True, keep='first')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data (GB)'] = df['data (GB)'].str.replace('GB', '').astype(float)\n",
    "df['partition (MB)'] = df['partition (MB)'].str.replace('MB', '').astype(int)\n",
    "df['num_worlds'] = ((df['data (GB)'] * 1024 + df['partition (MB)'] -1 ) // df['partition (MB)']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['partition (MB)', 'data (GB)', 'batch_size', 'batch_varying', 'run', 'num_worlds', 'total_duration (s)', 'inference_duration (s)']]\n",
    "df.sort_values(by=['partition (MB)', 'data (GB)','batch_size','batch_varying', 'run'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.round(2).to_csv('./results/state_machine_logs.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
